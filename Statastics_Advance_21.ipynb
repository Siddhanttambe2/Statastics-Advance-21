{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQkNw8/MS+uHSNc+pc+mt3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siddhanttambe2/Statastics-Advance-21/blob/main/Statastics_Advance_21.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. Explain the properties of the F-distribution."
      ],
      "metadata": {
        "id": "s-GOm0nRLGHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-Negative Values: The F-distribution is defined only for non-negative values because it’s based on the ratio of squared variances, which cannot be negative.\n",
        "\n",
        "Skewed Distribution: The F-distribution is right-skewed, particularly when the degrees of freedom are small. As the degrees of freedom increase, the distribution becomes less skewed and approaches a normal distribution.\n",
        "\n",
        "Asymmetry: Unlike symmetric distributions like the normal distribution, the F-distribution is asymmetric, with a tail that extends to the right.\n",
        "\n",
        "Degrees of Freedom: The shape of the F-distribution depends on two sets of degrees of freedom:\n",
        "d\n",
        "1\n",
        "​\n",
        "  (numerator degrees of freedom) and\n",
        "d\n",
        "2\n",
        "​\n",
        "  (denominator degrees of freedom). These degrees of freedom correspond to the variances being compared.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Parameter Sensitivity: Since the F-distribution relies on the ratio of two independent chi-squared distributed variables (each scaled by their degrees of freedom), it is sensitive to changes in\n",
        "d\n",
        "1\n",
        "​\n",
        "  and\n",
        "d\n",
        "2\n",
        "​\n",
        " , which affect the distribution’s shape.\n",
        "\n",
        "Application in Hypothesis Testing: The F-distribution is frequently used in hypothesis testing to compare variances (ANOVA) and test the overall significance of regression models."
      ],
      "metadata": {
        "id": "YbqcOh1KLLdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.In which types of statistical tests is the F-distribution used, and why is it appropriate for these tests?"
      ],
      "metadata": {
        "id": "gbGYBUhwTCCz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis of Variance (ANOVA): ANOVA tests are used to compare the means of three or more groups by analyzing the variance within each group and the variance between groups. The F-distribution is appropriate for ANOVA because it helps determine if the observed variances between groups are greater than would be expected by chance.\n",
        "\n",
        "Regression Analysis: In regression, the F-test assesses the overall significance of a model by testing whether the explained variance in the dependent variable by the model is significantly greater than the unexplained variance. The F-distribution is used to evaluate the ratio of these variances, indicating if the model provides a better fit than a model with no predictors.\n",
        "\n",
        "Comparing Variances: The F-test can compare the variances of two independent samples. This is useful when testing if two populations have the same variance. The F-distribution is appropriate here because it’s based on the ratio of two sample variances, which follows an F-distribution when the populations are normally distributed.\n",
        "\n",
        "\n",
        "\n",
        "The F-distribution is appropriate in these tests because it reflects the ratio of variances, capturing whether observed differences or model improvements are statistically significant by comparing variability explained by the model to random error."
      ],
      "metadata": {
        "id": "ImRtVkf7R3K3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What are the key assumptions required for conducting an F-test to compare the variances of two\n",
        "populations?"
      ],
      "metadata": {
        "id": "yScBeLhvS8WW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normality: The populations from which the samples are drawn should be normally distributed. The F-test is sensitive to deviations from normality, and non-normal data can lead to inaccurate results.\n",
        "\n",
        "Independence of Observations: The observations within each sample and between the two samples must be independent. This means the value of one observation should not influence the value of another.\n",
        "\n",
        "Random Sampling: The samples should be randomly selected from each population. Random sampling helps ensure that the samples represent the populations accurately.\n",
        "\n",
        "Ratio of Variances: The F-test assumes that the ratio of the variances is fixed if the null hypothesis (equal variances) is true. This allows the F-distribution to approximate the ratio of sample variances."
      ],
      "metadata": {
        "id": "N2bGOrTQSygm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the purpose of ANOVA, and how does it differ from a t-test?"
      ],
      "metadata": {
        "id": "mwcYso3lTR8m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Purpose of ANOVA: Analysis of Variance (ANOVA) is a statistical method used to test if there are significant differences among the means of three or more groups. By examining the variances within and between groups, ANOVA determines whether at least one group mean is significantly different from the others. It is widely used in experiments and studies with multiple groups to assess overall group effects.\n",
        "\n",
        "Differences from a T-Test:\n",
        "\n",
        "Number of Groups: The t-test is typically used to compare the means of only two groups. ANOVA, on the other hand, can compare the means of three or more groups simultaneously.\n",
        "\n",
        "Type of Variance Tested: In a t-test, the difference in means is directly tested, focusing on the variance between two specific groups. ANOVA compares the variance within groups to the variance between groups to assess overall differences among all groups.\n",
        "\n",
        "Error Rate Control: When comparing multiple groups, using multiple t-tests increases the risk of Type I errors (false positives). ANOVA controls for this error rate by testing all groups in a single analysis, making it a more efficient and reliable approach for multiple group comparisons."
      ],
      "metadata": {
        "id": "0KvWKtnpTgKT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Explain when and why you would use a one-way ANOVA instead of multiple t-tests when comparing more\n",
        "than two groups."
      ],
      "metadata": {
        "id": "xpCcdw5RTuZv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When to Use One-Way ANOVA: A one-way ANOVA is used when comparing the means of three or more independent groups on a single factor or independent variable. It’s appropriate in cases where there’s a need to test for overall differences across multiple groups in a single step, without having to make comparisons between each pair of groups individually.\n",
        "\n",
        "Why Use One-Way ANOVA Instead of Multiple T-Tests:\n",
        "\n",
        "Control for Type I Error: Conducting multiple t-tests increases the likelihood of a Type I error (incorrectly rejecting the null hypothesis). Each t-test carries its own error risk, so performing multiple tests compounds this risk. One-way ANOVA controls for this by testing all groups simultaneously in a single analysis.\n",
        "\n",
        "Efficiency: Running a single ANOVA test is more efficient than running multiple pairwise t-tests. If there are many groups, using multiple t-tests would be time-consuming and complex, while ANOVA provides a single result indicating whether any differences exist among the groups.\n",
        "\n",
        "Overall Comparison: One-way ANOVA provides an overall comparison among all groups, showing if at least one group mean is significantly different from the others. If the ANOVA result is significant, post hoc tests can then be used to identify which specific groups differ."
      ],
      "metadata": {
        "id": "ds9byZKFT7BE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain how variance is partitioned in ANOVA into between-group variance and within-group variance.\n",
        "How does this partitioning contribute to the calculation of the F-statistic?"
      ],
      "metadata": {
        "id": "hhdMYaJNUDFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In ANOVA, the total variance observed in the data is partitioned into two components:\n",
        "\n",
        "Between-Group Variance (or Variance Due to Treatment): This component represents the variation in the data that is due to differences between the group means. It reflects how much the group means differ from the overall mean. A large between-group variance suggests that there are substantial differences among the group means, potentially indicating a significant effect of the treatment or factor being tested.\n",
        "\n",
        "Within-Group Variance (or Variance Due to Error): This component represents the variation within each group, capturing individual differences and random error. It shows how much the observations within each group deviate from their respective group means. A small within-group variance implies that data points within each group are close to the group mean, while a large within-group variance indicates more variability within groups."
      ],
      "metadata": {
        "id": "bPwU-_8RUUXR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Compare the classical (frequentist) approach to ANOVA with the Bayesian approach. What are the key\n",
        "differences in terms of how they handle uncertainty, parameter estimation, and hypothesis testing?"
      ],
      "metadata": {
        "id": "KzvdR3b9UayP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The classical (frequentist) and Bayesian approaches to ANOVA differ fundamentally in how they handle uncertainty, estimate parameters, and test hypotheses:\n",
        "\n",
        "Handling Uncertainty:\n",
        "\n",
        "Frequentist Approach: In the classical ANOVA, uncertainty is quantified using probability distributions that assume the null hypothesis is true. The frequentist approach relies on p-values, which indicate the probability of observing data as extreme as, or more extreme than, the observed data if the null hypothesis is true.\n",
        "\n",
        "Bayesian Approach: Bayesian ANOVA incorporates uncertainty by using prior distributions, which represent initial beliefs about the parameters before observing the data. After observing the data, these priors are updated to form posterior distributions, reflecting the updated beliefs about the parameters. This approach allows for a more direct interpretation of uncertainty by evaluating the probability of hypotheses given the data.\n",
        "\n",
        "Parameter Estimation:\n",
        "\n",
        "Frequentist Approach: The classical ANOVA estimates parameters (such as group means) using point estimates, assuming that the sample provides a fixed value for each parameter. Variances are computed based on observed data, and confidence intervals give a range of plausible values for each parameter.\n",
        "\n",
        "Bayesian Approach: Bayesian ANOVA estimates parameters as distributions (posterior distributions), incorporating both prior knowledge and observed data. Each parameter has a probability distribution, offering a range of possible values with associated probabilities, making the Bayesian approach more flexible in incorporating prior knowledge and providing richer information about parameter uncertainty.\n",
        "\n",
        "Hypothesis Testing:\n",
        "\n",
        "Frequentist Approach: Hypothesis testing in classical ANOVA is based on significance testing, with the null hypothesis stating that all group means are equal. The F-test is used to compare within-group and between-group variance, yielding a p-value to decide whether to reject or fail to reject the null hypothesis. This approach provides a binary decision but does not assign probabilities to hypotheses.\n",
        "\n",
        "Bayesian Approach: In Bayesian ANOVA, hypothesis testing is done by comparing posterior probabilities of different hypotheses. The Bayesian approach allows direct computation of the probability of each hypothesis given the data. Additionally, Bayesian methods can provide Bayes factors, which measure the strength of evidence for one hypothesis over another, allowing for a graded assessment of evidence rather than a strict reject-or-fail-to-reject decision.\n",
        "Key Differences:\n",
        "\n",
        "Interpretation: Frequentist ANOVA offers results that are conditional on the null hypothesis, whereas Bayesian ANOVA provides results as probabilities of hypotheses given the data.\n",
        "\n",
        "Flexibility with Prior Knowledge: Bayesian ANOVA allows incorporation of prior beliefs or knowledge, while frequentist ANOVA does not.\n",
        "\n",
        "Result Presentation: Bayesian ANOVA provides a richer understanding of parameter uncertainty by presenting results as probability distributions, while frequentist ANOVA focuses on point estimates, confidence intervals, and p-values."
      ],
      "metadata": {
        "id": "nBes39bPUpUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Question: You have two sets of data representing the incomes of two different professions1\n",
        "\n",
        "V Profession A: [48, 52, 55, 60, 62]\n",
        "\n",
        "V Profession B: [45, 50, 55, 52, 47]\n",
        "\n",
        "Perform an F-test to determine if the variances of the two professions'\n",
        "incomes are equal. What are your conclusions based on the F-test?\n",
        "\n",
        "Task: Use Python to calculate the F-statistic and p-value for the given data.\n",
        "\n",
        "Objective: Gain experience in performing F-tests and interpreting the results in terms of variance comparison."
      ],
      "metadata": {
        "id": "znpf-XfZVP8g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Define the two sets of data\n",
        "profession_a = [48, 52, 55, 60, 62]\n",
        "profession_b = [45, 50, 55, 52, 47]\n",
        "\n",
        "# Perform the F-test\n",
        "f_statistic, p_value = stats.f_oneway(profession_a, profession_b)\n",
        "\n",
        "# Print the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: The variances of the two professions' incomes are not equal.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is not enough evidence to conclude that the variances are different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8_8aJjoS01m",
        "outputId": "9b5b5cd7-f450-4c47-be04-529e23d12137"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 3.232989690721649\n",
            "p-value: 0.10987970118946545\n",
            "Fail to reject the null hypothesis: There is not enough evidence to conclude that the variances are different.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the p-value (0.109) is greater than 0.05, we fail to reject the null hypothesis. The variances in incomes between Profession A and Profession B are not significantly different"
      ],
      "metadata": {
        "id": "cCnrKGp6WeKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Conduct a one-way ANOVA to test whether there are any statistically significant differences in\n",
        "average heights between three different regions with the following data1\n",
        "V Region A: [160, 162, 165, 158, 164'\n",
        "V Region B: [172, 175, 170, 168, 174'\n",
        "V Region C: [180, 182, 179, 185, 183'\n",
        "V Task: Write Python code to perform the one-way ANOVA and interpret the results\n",
        "V Objective: Learn how to perform one-way ANOVA using Python and interpret F-statistic and p-value"
      ],
      "metadata": {
        "id": "TYVM9P_-Wyml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Define the data for each region\n",
        "region_a = [160, 162, 165, 158, 164]\n",
        "region_b = [172, 175, 170, 168, 174]\n",
        "region_c = [180, 182, 179, 185, 183]\n",
        "\n",
        "# Perform one-way ANOVA\n",
        "f_statistic, p_value = stats.f_oneway(region_a, region_b, region_c)\n",
        "\n",
        "# Print the results\n",
        "print(\"F-statistic:\", f_statistic)\n",
        "print(\"p-value:\", p_value)\n",
        "\n",
        "# Interpret the results\n",
        "alpha = 0.05  # Significance level\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis: There is a significant difference in average heights between the three regions.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis: There is not enough evidence to conclude that the average heights are different.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOMoipR6WHme",
        "outputId": "a5996ca7-5318-4f10-9002-b1af295a3038"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F-statistic: 67.87330316742101\n",
            "p-value: 2.870664187937026e-07\n",
            "Reject the null hypothesis: There is a significant difference in average heights between the three regions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n_mHSWYCW0S7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}